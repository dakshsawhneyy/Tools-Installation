#! Tells fluentd, where to fetch logs from
<source>
    # read file line by line
    @type tail  

     # Actual Path of logs inside docker container
    path /var/lib/docker/containers/*/*-json.log   

    # Used for filtering -- add tag to each log
    tag docker.*    

    # Docker writes in json files
    format json     

    # Acts as a bookmark, to not read duplicate data again
    pos_file /fluentd/log/docker-container.pos      
</source>


#! Filter Logs fetched from container
# Apply filter on all logs with tag docker.*
<filter docker.**>
    # takes raw log strings and parse them into structured fields -- extracting actual useful items
    @type parser

    # Docker writes in json files
    format json

    # Field in json file which contains actual log message is inside log:
    key_name log
</filter>


# new filter to add timestamp to each log entry
<filter docker.**>
    # this is fluent plugin that can add, remove or update log entries
    @type record_transformer

    # Add timestamp 
    <record>
        @timestamp ${record["timestamp"]}
    </record>

</filter>


#! Tells fluentd, where to send logs to (elasticsearch)
<match docker.**>
    # sending to elastic search, type should be elastic search
    @type elasticsearch

    # elasticsearch host name -- docker compose services names are case insensitive
    host elasticsearch

    # elastic search port
    port 9200

    # Make it compatible with kibana
    logstash_format true

    # Add fluentd tag inside each log entry as new field
    include_tag_key true

    # Fluentd buffer logs and sends them to ElasticSearch every 5 seconds
    flush_interval 5s

    # every index can only have one type, and that type must be _doc.
    type_name _doc
</match>
